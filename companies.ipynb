{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                             2. extracting companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import itertools\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the corpus and the companies names csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('all_news.txt','r') as an:\n",
    "    corpus = ast.literal_eval(an.read())\n",
    "assert len(corpus) == 730"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(itertools.chain(*corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('train_set.txt','r') as tf:\n",
    "    train_set = ast.literal_eval(tf.read())\n",
    "with open('validate_set.txt','r') as vf:\n",
    "    valid_set = ast.literal_eval(vf.read())\n",
    "with open('test_set.txt','r') as tef:\n",
    "    test_set = ast.literal_eval(tef.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_company(company):\n",
    "    \n",
    "    company = company.strip()\n",
    "    \n",
    "    if len(company) == 0:\n",
    "        return False\n",
    "    else:\n",
    "        return company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "company_path = 'all/companies.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_file = open(company_path, 'r', encoding ='latin-1').readlines()\n",
    "companies = map(lambda x: process_company(x), company_file)\n",
    "companies = list(set(companies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_sw(name):\n",
    "    for word in stop_words:\n",
    "        if word in name.lower().split(): \n",
    "            return False\n",
    "\n",
    "    return True\n",
    "def remove_sw(name):\n",
    "    modified_name = [ ]\n",
    "    for word in name.split():\n",
    "        if word.lower() not in stop_words:\n",
    "            modified_name.append(word)\n",
    "            \n",
    "    return \" \".join(modified_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def find_names(article):\n",
    "    \n",
    "    regulation = \"(?:(?:[A-Z]+[a-z]*) ?)+\"\n",
    "    pattern = re.compile(regulation)\n",
    "    \n",
    "    names = map(lambda s: re.findall(pattern, s), article)\n",
    "    names = filter(None, names)\n",
    "    names = itertools.chain(*names)\n",
    "    \n",
    "    \n",
    "    names = map(lambda n: n.strip(), names)\n",
    "    names = filter(check_sw, names)\n",
    "    names = filter(lambda i: len(i) >= 4, names)\n",
    "    names = set(names)\n",
    "    \n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "companies = map(lambda x: remove_sw(x), companies)\n",
    "companies = list(companies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(names, article):\n",
    "    \n",
    "    features =[]\n",
    "    def check_in_sentence(name, sentence):\n",
    "        return ((\" \" + name + \" \" in sentence)\n",
    "             or (sentence[:len(name)+1] == name + \" \")\n",
    "             or (sentence[-len(name)+1:] == \" \" + name))\n",
    "    \n",
    "    for name in names:\n",
    "        sentences = map(lambda x: check_in_sentence(name, x), article)\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            feature = {'name':name}\n",
    "            \n",
    "            name_sp = name.split(' ')\n",
    "            first_word = name_sp[0]\n",
    "            last_word = name_sp[-1]\n",
    "            feature['first_name_length'] = len(first_word)\n",
    "            feature['last_name_length'] = len(last_word)\n",
    "            \n",
    "            sen = re.sub(r\"[^A-Za-z0-9 ]\", \" \", sentence)\n",
    "            tk = TweetTokenizer()\n",
    "            words = tk.tokenize(sen)\n",
    "            first_word_index = words.index(first_word)\n",
    "            last_word_index = words.index(last_word)\n",
    "            \n",
    "            pos_words = nltk.pos_tag(words)\n",
    "            feature['first_word_tag'] = pos_words[first_word_index][1]\n",
    "            feature['last_word_tag'] = pos_words[last_word_index][1]\n",
    "            \n",
    "            number_of_Cap = sum(1 for Cap in name if Cap.istitle())\n",
    "            feature['number_of_Cap'] = number_of_Cap\n",
    "            prob_of_Cap = number_of_Cap/len(name)\n",
    "            feature['Prob_of_Cap'] = prob_of_Cap\n",
    "            feature['is_company'] = int(name in companies)\n",
    "            \n",
    "            valuable_words = {'Company','Inc','Group','Corporation','Co','Corp','Capital','Management','Ltd'}\n",
    "            \n",
    "            for value_word in valueable_words:\n",
    "                if value_word in name_sp:\n",
    "                    feature[\"name_keyword\"] = True\n",
    "                    break\n",
    "                else: \n",
    "                    feature[\"name_keyword\"] = False\n",
    "            \n",
    "            features.append(feature)\n",
    "    return features  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "train =[]\n",
    "for i, article in enumerate(train_set):\n",
    "    names = find_names(article)\n",
    "#     print(list(names))\n",
    "    features = get_features(names,article)\n",
    "    \n",
    "    train.extend(features)\n",
    "    \n",
    "    if i % 500 == 0: \n",
    "        print(f\"This program has processed {i} articles\")\n",
    "    \n",
    "df_train = pd.DataFrame(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import resample\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train['is_company'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_positive = df_train[df_train.is_company==1]\n",
    "df_negative = df_train[df_train.is_company==0]\n",
    "resample_class = len(df_positive) * 10\n",
    "df_negative = resample(df_negative, replace = False, n_samples = resample_class)\n",
    "df_positive = resample(df_positive, replace = True, n_samples = resample_class)\n",
    "df_train2 = pd.concat([df_negative, df_positive])\n",
    "df_train2.first_name_pos = pd.factorize(df_train2.first_word_pos)[0]\n",
    "df_train2.last_name_pos = pd.factorize(df_train2.last_word_pos)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RFC = RandomForestClassifier(n_estimators = 20)\n",
    "featuress = [i for i in df_train2.columns if i not in (\"is_company\", \"name\")]\n",
    "RFC.fit(df_train2[featuress], df_train2[\"is_company\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = [ ]\n",
    "for i, article in enumerate(test_set):\n",
    "    names = find_names(article)\n",
    "    features = get_features(names, article)\n",
    "    test.extend(features)\n",
    "    \n",
    "    if i % 1000 == 0: print(f\"The program has processed {i} articles\")\n",
    "    \n",
    "dftest = pd.DataFrame(test)\n",
    "dftest.first_name_pos = pd.factorize(dftest.first_word_pos)[0]\n",
    "dftest.last_name_pos = pd.factorize(dftest.last_word_pos)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict = RFC.predict(dftest[featuress])\n",
    "get_metrics(dftest.is_ceo, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predt = RFC.predict(df_train2[featuress])\n",
    "predv = RFC.predict(dfv[featuress])\n",
    "preds = RFC.predict(dftest[featuress])\n",
    "\n",
    "\n",
    "found_companiess = [ ]\n",
    "found_companies.extend(list(df_train2.iloc[np.where(predt == 1)].name))\n",
    "found_companies.extend(list(dfv.iloc[np.where(predv == 1)].name))\n",
    "found_companies.extend(list(dftest.iloc[np.where(preds == 1)].name))\n",
    "\n",
    "\n",
    "with open(\"found_companies.txt\", \"w\") as f:\n",
    "    for ceo in found_comanies:\n",
    "        f.write(company)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
