{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                          1.Extracting CEO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load into dataset and get the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ceo_path = 'all/ceo.csv'\n",
    "percent_path = 'all/percentage.csv'\n",
    "company_path = 'all/companies.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_ceo (name):\n",
    "    \n",
    "    name = name.strip().split(',')\n",
    "    name = filter(None, name)\n",
    "    name = list(map(lambda x: x.strip(),name))\n",
    "    if len(name) == 0:\n",
    "        return False\n",
    "    elif len(name) == 1:\n",
    "        return name[0]\n",
    "    elif len(name) == 2:\n",
    "        return name[0] + ' ' + name[1]\n",
    "    else:\n",
    "        return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ceo_file = open(ceo_path,'r', encoding = 'latin-1').readlines()\n",
    "ceo_name = map(lambda x: process_ceo(x), ceo_file)\n",
    "ceo_name = list(set(ceo_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_percent(percent):\n",
    "    \n",
    "    percent = percent.strip().replace('\"','').split(' ')\n",
    "    percent = filter(None, percent)\n",
    "    percent = list(percent)\n",
    "    \n",
    "    if len(percent) == 0:\n",
    "        return False\n",
    "    elif len(percent) == 1:\n",
    "        return percent[0]\n",
    "    elif len(percent) >= 2:\n",
    "        return percent[0] + ' ' + percent[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "percent_file = open(percent_path, 'r', encoding = 'latin-1').readlines()\n",
    "percent = map(lambda x: process_percent(x), percent_file)\n",
    "percent = list(set(percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_company(company):\n",
    "    \n",
    "    company = company.strip()\n",
    "    \n",
    "    if len(company) == 0:\n",
    "        return False\n",
    "    else:\n",
    "        return company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "company_file = open(company_path, 'r', encoding ='latin-1').readlines()\n",
    "companies = map(lambda x: process_company(x), company_file)\n",
    "companies = list(set(companies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load into corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_2013 = '2013'\n",
    "path_2014 = '2014'\n",
    "path_ceo ='all/ceo.csv'\n",
    "path_percent = 'all/percentage.csv'\n",
    "path_companies = 'all/companies.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_files(dir):\n",
    "    files = glob.glob(dir + '/*')\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files_2013 = get_files(path_2013)\n",
    "files_2014 = get_files(path_2014)\n",
    "files = files_2013+files_2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_corpus(files):\n",
    "    corpus =[]\n",
    "    for i, file in enumerate(files):\n",
    "        with open(file, 'r',encoding = 'latin-1') as f:\n",
    "            articles =[]\n",
    "            for article in f:\n",
    "                article = sent_tokenize(article)\n",
    "                articles.append(article)\n",
    "        corpus.append(articles)\n",
    "        \n",
    "        if i % 100 ==0:\n",
    "            print(f'have already read {i} in total')\n",
    "    \n",
    "    with open('all_news.txt','w') as an:\n",
    "        an.write(str(corpus))\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have already read 0 in total\n",
      "have already read 100 in total\n",
      "have already read 200 in total\n",
      "have already read 300 in total\n",
      "have already read 400 in total\n",
      "have already read 500 in total\n",
      "have already read 600 in total\n",
      "have already read 700 in total\n"
     ]
    }
   ],
   "source": [
    "get_corpus(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('all_news.txt','r') as an:\n",
    "    corpus = ast.literal_eval(an.read())\n",
    "assert len(corpus) == 730"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = list(itertools.chain(*corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35898\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus))\n",
    "split_first = int(0.6 * len(corpus))\n",
    "split_second = int(0.8 * len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_set = corpus[0:split_first]\n",
    "validating_set = corpus[split_first:split_second]\n",
    "testing_set = corpus[split_second:]\n",
    "assert len(corpus) == len(training_set) + len(validating_set) + len(testing_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('training_final.txt', 'w') as tf:\n",
    "    tf.write(str(training_set))\n",
    "with open('validating_final.txt','w') as vf:\n",
    "    vf.write(str(validating_set))\n",
    "with open('testing_final.txt','w') as tef:\n",
    "    tef.write(str(testing_set))\n",
    "with open('train_set.txt','r') as tf:\n",
    "    train_set = ast.literal_eval(tf.read())\n",
    "with open('validate_set.txt','r') as vf:\n",
    "    valid_set = ast.literal_eval(vf.read())\n",
    "with open('test_set.txt','r') as tef:\n",
    "    test_set = ast.literal_eval(tef.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_sw(name):\n",
    "    for word in stop_words:\n",
    "        if word in name.lower().split(): \n",
    "            return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_notceo(name):\n",
    "    first_name, last_name = name.split()\n",
    "    \n",
    "    one_word_place = {'USA','US','England','Asia','China','Europe','Africa','Texas','Atlantic','Japan',\n",
    "                     'Singapore','Koera','Germany','France','North','East','West','South'}\n",
    "    if first_name in one_word_place or last_name in one_word_place:\n",
    "        return False\n",
    "    two_word_place = {'Los Angeles','San Francisco','United Kingdom','Wall Street','New York','Hong Kong',\n",
    "                     'Silicon Valley'}\n",
    "    if name in two_word_place:\n",
    "        return False\n",
    "    Financial = {'Economy','Economist','Economics','Federal','Management','Trading','Finance','Financial','Money'\n",
    "              ,'Debt','Balance','Environment'}\n",
    "    if first_name or last_name in Financial:\n",
    "        return False\n",
    "    policy = {'House','Government','Congress','Senator','Office','President','Budget','Policy',\n",
    "                'Congressman','Congresswoman','Majority','Minority','Politics'}\n",
    "    if first_name or last_name in policy:\n",
    "        return False\n",
    "    organisation = {'Corp','Inc','Ltd','Corporation','School','University','Bureau'}\n",
    "    if first_name or last_name in organisation:\n",
    "        return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_names(article):\n",
    "\n",
    "    names = map(lambda x : re.findall(r'[A-Z][a-z]* [A-Z][a-z]*',x),article)\n",
    "    names = filter(None, names)\n",
    "    names = list(itertools.chain(*names))\n",
    "\n",
    "    \n",
    "    names = filter(check_sw, names)\n",
    "    names = filter(check_notceo,names)\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_features(names, article):\n",
    "    \n",
    "    features =[]\n",
    "    \n",
    "#     def find_which_sentence(name, sentence):\n",
    "#         return ((\" \" + name + \" \") in sentence or (\" \" + name == sentence[-len(name)+1:]) or (name + \" \" == sentence[:len(name)+1]))\n",
    "    \n",
    "    def check_in_sentence(name, sentence):\n",
    "        \n",
    "        return ((\" \" + name + \" \" in sentence)\n",
    "             or (sentence[:len(name)+1] == name + \" \")\n",
    "             or (sentence[-len(name)+1:] == \" \" + name))\n",
    "   \n",
    "    for name in names:\n",
    "        sentences = filter(lambda x: check_in_sentence(name, x), article)\n",
    "        for sentence in sentences:\n",
    "            \n",
    "            \n",
    "            feature = { \"name\" : name }\n",
    "            \n",
    "            first_name, last_name = name.split(\" \")\n",
    "            feature['first_name_length'] = len(first_name)\n",
    "            feature['last_name_length'] = len(last_name)\n",
    "            \n",
    "            s = re.sub(r\"[^A-Za-z0-9 ]\", \" \", sentence)\n",
    "            tk = TweetTokenizer()\n",
    "            words = tk.tokenize(s)\n",
    "            \n",
    "            feature['first_name_position'] = words.index(first_name)\n",
    "            \n",
    "            \n",
    "            pos_words = nltk.pos_tag(words)\n",
    "            feature['first_name_pos'] = pos_words[words.index(first_name)][1]\n",
    "            feature['last_name_pos'] = pos_words[words.index(last_name)][1]\n",
    "            \n",
    "            before_word_index = words.index(first_name)-1\n",
    "            after_word_index = words.index(last_name) + 1\n",
    "            \n",
    "            if before_word_index < 0:\n",
    "                feature['before_word_Cap'] = 0\n",
    "                feature['before_word_Pos'] = 'NULL'\n",
    "            else:\n",
    "                feature['before_word_Cap'] = int(words[before_word_index].istitle())\n",
    "                feature['before_word_Pos'] = pos_words[before_word_index][1]\n",
    "            \n",
    "            if after_word_index >= len(words):\n",
    "                feature['after_word_Cap'] = 0\n",
    "                feature['after_word_Pos'] = 'NULL'\n",
    "            else:\n",
    "                feature['after_word_Cap']=int(words[after_word_index].istitle())\n",
    "                feature['after_word_Pos'] = pos_words[after_word_index][1]\n",
    "\n",
    "            \n",
    "            \n",
    "            feature['is_ceo'] = int(name in ceo_name)\n",
    "            \n",
    "            features.append(feature)\n",
    "    return features\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This program has processed 0 articles\n",
      "This program has processed 500 articles\n",
      "This program has processed 1000 articles\n",
      "This program has processed 1500 articles\n",
      "This program has processed 2000 articles\n",
      "This program has processed 2500 articles\n",
      "This program has processed 3000 articles\n",
      "This program has processed 3500 articles\n",
      "This program has processed 4000 articles\n",
      "This program has processed 4500 articles\n",
      "This program has processed 5000 articles\n",
      "This program has processed 5500 articles\n",
      "This program has processed 6000 articles\n",
      "This program has processed 6500 articles\n",
      "This program has processed 7000 articles\n",
      "This program has processed 7500 articles\n",
      "This program has processed 8000 articles\n",
      "This program has processed 8500 articles\n",
      "This program has processed 9000 articles\n",
      "This program has processed 9500 articles\n",
      "This program has processed 10000 articles\n",
      "This program has processed 10500 articles\n",
      "This program has processed 11000 articles\n",
      "This program has processed 11500 articles\n",
      "This program has processed 12000 articles\n",
      "This program has processed 12500 articles\n",
      "This program has processed 13000 articles\n",
      "This program has processed 13500 articles\n",
      "This program has processed 14000 articles\n",
      "This program has processed 14500 articles\n",
      "This program has processed 15000 articles\n",
      "This program has processed 15500 articles\n",
      "This program has processed 16000 articles\n",
      "This program has processed 16500 articles\n",
      "This program has processed 17000 articles\n",
      "This program has processed 17500 articles\n",
      "This program has processed 18000 articles\n",
      "This program has processed 18500 articles\n",
      "This program has processed 19000 articles\n",
      "This program has processed 19500 articles\n",
      "This program has processed 20000 articles\n",
      "This program has processed 20500 articles\n",
      "This program has processed 21000 articles\n",
      "This program has processed 21500 articles\n"
     ]
    }
   ],
   "source": [
    "train =[]\n",
    "for i, article in enumerate(train_set):\n",
    "    names = find_names(article)\n",
    "#     print(list(names))\n",
    "    features = get_features(names,article)\n",
    "    \n",
    "    train.extend(features)\n",
    "    \n",
    "    if i % 500 == 0: \n",
    "        print(f\"This program has processed {i} articles\")\n",
    "    \n",
    "df_train = pd.DataFrame(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-9b7c4a596534>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import resample\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    147443\n",
       "1      8348\n",
       "Name: is_ceo, dtype: int64"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['is_ceo'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_positive = df_train[df_train.is_ceo==1]\n",
    "df_negative = df_train[df_train.is_ceo==0]\n",
    "resample_class = len(df_positive) * 10\n",
    "df_negative = resample(df_negative, replace = False, n_samples = resample_class)\n",
    "df_positive = resample(df_positive, replace = True, n_samples = resample_class)\n",
    "df_train2 = pd.concat([df_negative, df_positive])\n",
    "df_train2.after_word_Pos = pd.factorize(df_train2.after_word_Pos)[0]\n",
    "df_train2.before_word_Pos = pd.factorize(df_train2.after_word_Pos)[0]\n",
    "df_train2.first_name_pos = pd.factorize(df_train2.first_name_pos)[0]\n",
    "df_train2.last_name_pos = pd.factorize(df_train2.last_name_pos)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=20, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RFC = RandomForestClassifier(n_estimators = 20)\n",
    "featuress = [i for i in df_train2.columns if i not in (\"is_ceo\", \"name\")]\n",
    "RFC.fit(df_train2[featuress], df_train2[\"is_ceo\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 articles\n",
      "Processed 1000 articles\n",
      "Processed 2000 articles\n",
      "Processed 3000 articles\n",
      "Processed 4000 articles\n",
      "Processed 5000 articles\n",
      "Processed 6000 articles\n",
      "Processed 7000 articles\n"
     ]
    }
   ],
   "source": [
    "validation= [ ]\n",
    "for i, article in enumerate(validating_set):\n",
    "    names = find_names(article)\n",
    "    features = get_features(names, article)\n",
    "    validation.extend(features)\n",
    "    \n",
    "    if i % 1000 == 0: print(f\"The program has processed {i} articles\")\n",
    "    \n",
    "dfv = pd.DataFrame(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7180"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validating_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfv.after_word_Pos = pd.factorize(dfv.after_word_Pos)[0]\n",
    "dfv.before_word_Pos = pd.factorize(dfv.after_word_Pos)[0]\n",
    "dfv.first_name_pos = pd.factorize(dfv.first_name_pos)[0]\n",
    "dfv.last_name_pos = pd.factorize(dfv.last_name_pos)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_metrics(truth, predicted):\n",
    "    \n",
    "    \n",
    "    confusion_matrix = metrics.confusion_matrix(truth, predicted)\n",
    "    accuracy = metrics.accuracy_score(truth, predicted)\n",
    "    precision = metrics.precision_score(truth, predicted)\n",
    "    recall = metrics.recall_score(truth, predicted)\n",
    "#     F1 = metrics.f1_score(truth, predicted)\n",
    "    \n",
    "    print(f\"Confusion Matrix:\\n {confusion_matrix}\\n\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 Score: {F1}\")\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[49985  4508]\n",
      " [ 1363   851]]\n",
      "\n",
      "Accuracy: 0.8964678082070997\n",
      "Precision: 0.15879828326180256\n",
      "Recall: 0.3843721770551039\n",
      "F1 Score: 0.22474580747392048\n"
     ]
    }
   ],
   "source": [
    "predict = RFC.predict(dfv[featuress])\n",
    "get_metrics(dfv.is_ceo, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The program has processed 0 articles\n",
      "The program has processed 1000 articles\n",
      "The program has processed 2000 articles\n",
      "The program has processed 3000 articles\n",
      "The program has processed 4000 articles\n",
      "The program has processed 5000 articles\n",
      "The program has processed 6000 articles\n",
      "The program has processed 7000 articles\n"
     ]
    }
   ],
   "source": [
    "test = [ ]\n",
    "for i, article in enumerate(test_set):\n",
    "    names = find_names(article)\n",
    "    features = get_features(names, article)\n",
    "    test.extend(features)\n",
    "    \n",
    "    if i % 1000 == 0: print(f\"The program has processed {i} articles\")\n",
    "    \n",
    "dftest = pd.DataFrame(test)\n",
    "dftest.after_word_Pos = pd.factorize(dftest.after_word_Pos)[0]\n",
    "dftest.before_word_Pos = pd.factorize(dftest.after_word_Pos)[0]\n",
    "dftest.first_name_pos = pd.factorize(dftest.first_name_pos)[0]\n",
    "dftest.last_name_pos = pd.factorize(dftest.last_name_pos)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[46697  5786]\n",
      " [ 1659   989]]\n",
      "\n",
      "Accuracy: 0.8649580091055848\n",
      "Precision: 0.14597785977859778\n",
      "Recall: 0.3734894259818731\n",
      "F1 Score: 0.20991191764830733\n"
     ]
    }
   ],
   "source": [
    "predict = RFC.predict(dftest[featuress])\n",
    "get_metrics(dftest.is_ceo, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predt = RFC.predict(df_train2[featuress])\n",
    "predv = RFC.predict(dfv[featuress])\n",
    "preds = RFC.predict(dftest[featuress])\n",
    "\n",
    "\n",
    "found_ceos = [ ]\n",
    "found_ceos.extend(list(df_train2.iloc[np.where(predt == 1)].name))\n",
    "found_ceos.extend(list(dfv.iloc[np.where(predv == 1)].name))\n",
    "found_ceos.extend(list(dftest.iloc[np.where(preds == 1)].name))\n",
    "\n",
    "\n",
    "with open(\"found_ceos.txt\", \"w\") as f:\n",
    "    for ceo in found_ceos:\n",
    "        f.write(ceo + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                            2. Extracting percent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
