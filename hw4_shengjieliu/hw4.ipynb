{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import ast\n",
    "import itertools\n",
    "import nltk\n",
    "import elasticsearch\n",
    "from sklearn import tree\n",
    "from nltk.tree import Tree\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# typing your question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "script, question = sys.argv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# script, question = sys.argv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get our corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('all_news.txt','r') as f:\n",
    "    corpus = ast.literal_eval(f.read())\n",
    "    corpus = list(itertools.chain(*corpus))\n",
    "    corpus = list(itertools.chain(*corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# connect the elasticsearch local server and indexing the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "from elasticsearch.helpers import bulk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client = Elasticsearch()\n",
    "documents = []\n",
    "\n",
    "for i in range(len(corpus)):\n",
    "    process_sent = corpus[i]\n",
    "    \n",
    "    document = {\n",
    "        \"_index\" : \"articles-index\",\n",
    "        \"_type\" : \"articles\",\n",
    "        \"_id\" : i,\n",
    "        \"_source\" : {\n",
    "            \"any\" : \"data\" + str(i),\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"body\" : process_sent\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    documents.append(document)\n",
    "if len(documents) > 0:\n",
    "    bulk(client, documents)\n",
    "else:\n",
    "    raise Exception('There is no document')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Question Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1.define question classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "question_1 = 'which companies went bankrupt in month x of year y?'\n",
    "question_2 = 'what affects gdp?'\n",
    "question_3 = 'what percentage of drop or increase change in gdp is associated with z?'\n",
    "question_4 = 'Who is the ceo of company x?'\n",
    "questions =[question_1, question_2, question_3, question_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WORD = re.compile(r'\\w+')\n",
    "def get_cosine(vec1, vec2):\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "    \n",
    "    sum1 = sum([vec1[x] ** 2 for x in vec1.keys()])\n",
    "    sum2 = sum([vec2[x] ** 2 for x in vec2.keys()])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "    \n",
    "    if not denominator: \n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "def text_to_vector(text):\n",
    "    words = WORD.findall(text)\n",
    "    return Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vector_1 = text_to_vector(question_1)\n",
    "vector_2 = text_to_vector(question_2)\n",
    "vector_3 = text_to_vector(question_3)\n",
    "vector_4 = text_to_vector(question_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def question_classifier(question):\n",
    "    question = text_to_vector(question.lower())\n",
    "    cosine_similarity = np.zeros(4)\n",
    "    cosine_similarity[0] = get_cosine(vector_1, question)\n",
    "    cosine_similarity[1] = get_cosine(vector_2, question)\n",
    "    cosine_similarity[2] = get_cosine(vector_3, question)\n",
    "    cosine_similarity[3] = get_cosine(vector_4, question)\n",
    "    \n",
    "    max_index = np.argmax(cosine_similarity)\n",
    "    \n",
    "    return max_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2. extract entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_entities(sen):\n",
    "    if type(sen) == str:\n",
    "         NE = nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sen)))\n",
    "    else:\n",
    "         NE = nltk.ne_chunk(nltk.pos_tag(sen))\n",
    "    entities = []\n",
    "    curr = []\n",
    "    \n",
    "    for i in NE:\n",
    "        if type(i) == Tree:\n",
    "            curr.append(\" \".join([token for token, pos in i.leaves()]))\n",
    "        elif curr:\n",
    "            ne = \" \".join(curr)\n",
    "            if ne not in entities:\n",
    "                entities.append(ne)\n",
    "                curr =[]\n",
    "        else:\n",
    "            continue\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2. define GDP question "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GDP_question(question):\n",
    "    \n",
    "    token_question = nltk.word_tokenize(question)\n",
    "    if '?' in token_question:\n",
    "        token_question.remove('?')\n",
    "    pos_question = nltk.pos_tag(token_question)\n",
    "    \n",
    "    keywords = []\n",
    "    for i in range(len(token_question)-1,-1,-1):\n",
    "        if pos_question[i][1] != 'TO' and pos_question[i][1] !='IN':\n",
    "            keywords.insert(0,token_question[i])\n",
    "        else:\n",
    "            break\n",
    "    ESquery = ['GDP','increase','decrease','growth','effect','effects','affect','affects','%','percent']\n",
    "    for i in range(len(keywords)):\n",
    "        ESquery.append(keywords[i])\n",
    "    ESsearch = client.search(index = 'articles-index', q = ESquery, size =1000)\n",
    "    EShits = ESsearch['hits']['hits']\n",
    "    \n",
    "    CandSen = []\n",
    "    CandTok = []\n",
    "    CandSco = []\n",
    "    \n",
    "    check = ESquery\n",
    "    check.remove('GDP')\n",
    "    for i in np.arange(len(EShits)):\n",
    "        sentence = EShits[i]['_source']['body']\n",
    "        token_sen = nltk.word_tokenize(sentence)\n",
    "        score = 0\n",
    "        if 'GDP' in token_sen:\n",
    "            score += 2\n",
    "        for i in np.arange(len(keywords)):\n",
    "            if keywords[i] in token_sen:\n",
    "                score += 2\n",
    "        for i in np.arange(len(check)):\n",
    "            if check[i] in token_sen:\n",
    "                score += 1\n",
    "        CandSen.append(sentence)\n",
    "        \n",
    "        CandTok.append(token_sen)\n",
    "        CandSco.append(score)\n",
    "    CanDF = pd.DataFrame(data = [CandSen, CandTok, CandSco], index = ['Sen','Token','Score'])\n",
    "    CanDF = CanDF.transpose()\n",
    "    CanDF = CanDF.sort_values(['Score'],ascending = False)\n",
    "    CanDF.index = range(len(CanDF['Sen']))\n",
    "    \n",
    "    percents = ['%','percent','percentage']\n",
    "    bestsen =[]\n",
    "    bestscore = []\n",
    "    bestanswers = []\n",
    "    \n",
    "    for i in range(3):\n",
    "        bestsen.append(CanDF['Token'][i])\n",
    "        score = np.zeros(len(bestsen[i]))\n",
    "        for j in range(len(score)):\n",
    "            GDPleft = 100\n",
    "            GDPright = 100\n",
    "            targLeft = 100\n",
    "            targRight = 100\n",
    "            is_percent = 0\n",
    "            \n",
    "            if bestsen[i][j] in percents:\n",
    "                is_percent = 1\n",
    "            count = 0\n",
    "            for k in range(j, -1, -1):\n",
    "                if bestsen[i][k] == 'GDP':\n",
    "                    GDPleft == count\n",
    "                    break\n",
    "                else:\n",
    "                    count += 1\n",
    "            count = 0\n",
    "            for k in range(j, -1, -1):\n",
    "                if bestsen[i][k] == keywords[0]:\n",
    "                    targLeft = count\n",
    "                    break\n",
    "                else:\n",
    "                    count += 1\n",
    "            count = 0\n",
    "            for k in range(j, len(bestsen[i]), 1):\n",
    "                if bestsen[i][k] == 'GDP':\n",
    "                    GDPright = count\n",
    "                    break\n",
    "                else:\n",
    "                    count += 1\n",
    "            count = 0\n",
    "            for k in range(j, len(bestsen[i]), 1):\n",
    "                if bestsen[i][k] == keywords[0]:\n",
    "                    targRight = count\n",
    "                    break\n",
    "                else:\n",
    "                    count += 1\n",
    "            GDPdist = min([GDPleft, GDPright])\n",
    "            targdist = min([targLeft, targRight])\n",
    "            score[j] = is_percent * 500 - GDPdist - targdist\n",
    "            \n",
    "        bestscore.append(score)\n",
    "        index, value = max(enumerate(bestscore[i]), key = operator.itemgetter(1))\n",
    "        answer = [bestsen[i][index-1], bestsen[i][index]]\n",
    "        bestanswers.append(answer)\n",
    "        \n",
    "    for i in range(3):\n",
    "        if bestanswers[i][1] in percents:\n",
    "            bestanswers[i][1] = 'percent'\n",
    "            output = bestanswers[0][0] + \" \"\n",
    "            output += bestanswers[0][1]\n",
    "            break\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Bankrupt_question(question):\n",
    "    token_question = nltk.word_tokenize(question)\n",
    "    token_question = [token for token in token_question if not token in stop_words]\n",
    "    pos_question = nltk.pos_tag(token_question)\n",
    "    \n",
    "    ESquery = ['bankrupt','declared','filed','bankruptcy',]\n",
    "    \n",
    "    date = []\n",
    "    for i in range(len(token_question)):\n",
    "        pos = pos_question[i][1]\n",
    "        \n",
    "        if pos == 'NNP' or pos == 'CD':\n",
    "            ESquery.append(pos_question[i][0])\n",
    "            date.append(pos_question[i][0])\n",
    "    ESquery.append\n",
    "    ESsearch = client.search(index = 'articles-index', q = ESquery, size = 1000)\n",
    "    CandSen = []\n",
    "    CandTok = []\n",
    "    EShits = ESsearch['hits']['hits']\n",
    "    for i in np.arange(len(EShits)):\n",
    "        sentence = EShits[i]['_source']['body']\n",
    "        token_sentence = nltk.word_tokenize(sentence)\n",
    "        token_sentence = [token for token in token_sentence if not token in stop_words]\n",
    "        \n",
    "        Date_inside = 1\n",
    "        Bank_inside = 0\n",
    "        Bank = ['bankruptcy','bankrupt']\n",
    "        \n",
    "        for i in range(len(date)):\n",
    "            if date[i] not in token_sentence:\n",
    "                Date_inside = 0\n",
    "        for i in range(len(Bank)):\n",
    "            if Bank[i] in token_sentence:\n",
    "                Bank_inside = 1\n",
    "        if Date_inside == 1 and Bank_inside ==1:\n",
    "            CandSen.append(sentence)\n",
    "            CandTok.append(token_sentence)\n",
    "    candidates = []\n",
    "    for i in np.arange(len(CandSen)):\n",
    "        ne = extract_entities(CandSen[i])\n",
    "        for j in np.arange(len(ne)):\n",
    "            candidates.append(ne[j])\n",
    "    if candidates != []:\n",
    "        return max(set(candidates), key = candidates.count)\n",
    "    else:\n",
    "        print('no results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CEO_question(question):\n",
    "    ne = extract_entities(question)\n",
    "    ESsearch = client.search(index ='articles-index', q = ne, size = 1000)\n",
    "    Candsen =[]\n",
    "    for i in np.arange(len(ESsearch['hits']['hits'])):\n",
    "        Candsen.append(ESsearch['hits']['hits'][i]['_source']['body'])\n",
    "        Candsen[i] = nltk.word_tokenize(Candsen[i])\n",
    "    tq = nltk.word_tokenize(ne[0])\n",
    "    if 'of' in tq:\n",
    "        tq.remove('of')\n",
    "    if len(tq) == 1:\n",
    "        companyname = tq[0]\n",
    "    else:\n",
    "        companyname = tq[1]\n",
    "    if len(companyname) > 2:\n",
    "        for i in range(2, len(tq)):\n",
    "            companyname = companyname + ' ' + tq[i]\n",
    "    rs =[]\n",
    "    for i in np.arange(len(Candsen)):\n",
    "        t = 1\n",
    "        for j in np.arange(len(tq)):\n",
    "            if tq[j] not in Candsen[i]:\n",
    "                t = 0\n",
    "        if t == 1:\n",
    "            rs.append(Candsen[i])\n",
    "    Candidates = []\n",
    "    for i in np.arange(len(rs)):\n",
    "        ce = extract_entities(rs[i])\n",
    "        for j in np.arange(len(ce)):\n",
    "            if (ce[j] != companyname) and (len(nltk.word_tokenize(ce[j]))) == 2:\n",
    "                Candidates.append(ce[j])\n",
    "    if Candidates != []:\n",
    "        return max(set(Candidates), key = Candidates.count)\n",
    "    else:\n",
    "        print('No results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def answer(question) :\n",
    "    questiontype = question_classifier(question)\n",
    "    \n",
    "    if questiontype == 0 :\n",
    "        return Bankrupt_question(question)\n",
    "    elif questiontype == 1 :\n",
    "        return(\"Consumption, consumer spending, government spending, investment, imports, exports, foreign trade\")\n",
    "    elif questiontype == 2 :\n",
    "        return GDP_question(question)\n",
    "    elif questiontype == 3 :\n",
    "        return CEO_question(question) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jack Dorsey'"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer(\"What is the CEO of Twitter?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Steve Ballmer'"
      ]
     },
     "execution_count": 551,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer(\"What is the CEO of Microsoft?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mark Zuckerberg'"
      ]
     },
     "execution_count": 552,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer(\"What is the CEO of Facebook?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lehman Brothers'"
      ]
     },
     "execution_count": 554,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer(\"Which company went bankrupt in September 2008?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Detroit'"
      ]
     },
     "execution_count": 561,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer(\"Which company went bankrupt in July 2013?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reuters'"
      ]
     },
     "execution_count": 572,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer(\"Which company went bankrupt in October 2014?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Consumption, consumer spending, government spending, investment, imports, exports, foreign trade'"
      ]
     },
     "execution_count": 573,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer(\"What factors have the most effect on GDP?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.7 percent'"
      ]
     },
     "execution_count": 574,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer(\"what percentage is assoicated with foreign trade?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.6 percent'"
      ]
     },
     "execution_count": 578,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer(\"what percentage change in GDP results from government spending?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
